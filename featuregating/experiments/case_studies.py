"""
Qualitative Case Study Analysis.

Traces improvements in top-performing images back to specific SAE features
and their semantics. Identifies which features boosted or suppressed attribution
in top-improved images, and visualizes feature prototypes from a reference set.
"""

import json
import re
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from PIL import Image

def _discover_default_experiment(dataset: str) -> Tuple[Path, str]:
    """Find a recent sweep directory and first non-vanilla config for ``dataset``."""
    run_dirs = sorted(
        [p for p in Path("data/runs").glob("feature_gradient_sweep_*") if p.is_dir()],
        key=lambda p: p.name,
        reverse=True,
    )
    for run_dir in run_dirs:
        dataset_dir = run_dir / dataset
        if not dataset_dir.exists():
            continue
        configs = sorted(
            [p.name for p in dataset_dir.iterdir() if p.is_dir() and p.name != "vanilla"]
        )
        if configs:
            return run_dir, configs[0]

    raise FileNotFoundError(
        f"No suitable experiment found for dataset '{dataset}' under data/runs/.\n"
        "Run a sweep first with `uv run python -m featuregating.experiments.sweep`."
    )


def _infer_layers_from_experiment_name(experiment_config: str) -> List[int]:
    """Infer layer list from names like ``layers_6_7_8_kappa_...``."""
    match = re.match(r"^layers_([0-9_]+)_kappa_", experiment_config)
    if not match:
        return [3]
    return [int(x) for x in match.group(1).split("_") if x.isdigit()]


def _validate_case_study_inputs(vanilla_path: Path, gated_path: Path, layers: List[int]) -> None:
    """Fail fast with actionable errors before heavy analysis work starts."""
    missing: List[str] = []

    if not vanilla_path.exists():
        missing.append(str(vanilla_path))
    if not gated_path.exists():
        missing.append(str(gated_path))
    if missing:
        raise FileNotFoundError(
            "Required experiment paths are missing:\n"
            + "\n".join(f"  - {m}" for m in missing)
        )

    if not list(vanilla_path.glob("faithfulness_stats_*.json")):
        raise FileNotFoundError(
            f"Missing faithfulness JSON in {vanilla_path}.\n"
            "Expected file pattern: faithfulness_stats_*.json"
        )
    if not list(gated_path.glob("faithfulness_stats_*.json")):
        raise FileNotFoundError(
            f"Missing faithfulness JSON in {gated_path}.\n"
            "Expected file pattern: faithfulness_stats_*.json"
        )

    debug_dir = gated_path / "debug"
    if not debug_dir.exists():
        raise FileNotFoundError(
            f"Missing debug directory: {debug_dir}\n"
            "Case studies require layer debug NPZ files generated by the pipeline."
        )

    for layer_idx in layers:
        expected = debug_dir / f"layer_{layer_idx}_debug.npz"
        if not expected.exists():
            raise FileNotFoundError(
                f"Missing debug NPZ for requested layer: {expected}\n"
                "Re-run the sweep with matching layers or update the case-study layer list."
            )


def _resolve_validation_activations_path(dataset: str, layers: List[int]) -> Path:
    """Return validation activation path, extracting once if needed."""
    from featuregating.models.sae_extraction import extract_sae_activations_if_needed

    return extract_sae_activations_if_needed(
        dataset_name=dataset,
        layers=layers,
        split='val',
        output_dir=Path(f"./data/sae_activations/{dataset}_val"),
        subset_size=None,
        use_clip=True,
    )


def _patch_grid(n_patches: int) -> Tuple[int, int]:
    """Derive (patches_per_side, patch_size) from total patch count."""
    patches_per_side = int(np.sqrt(n_patches))
    patch_size = 224 // patches_per_side
    return patches_per_side, patch_size


def _patch_coords(patch_idx: int, patches_per_side: int, patch_size: int) -> Tuple[int, int]:
    """Convert linear patch index to (x, y) pixel coordinates."""
    row = patch_idx // patches_per_side
    col = patch_idx % patches_per_side
    return col * patch_size, row * patch_size


def _save_attribution_overlay(
    img_array: np.ndarray,
    save_path: Path,
    attribution: Optional[np.ndarray] = None,
    highlight: Optional[Tuple[int, int, int, int]] = None,
    highlight_color: str = 'lime',
    highlight_linewidth: int = 3,
    title: Optional[str] = None,
):
    """Save image with optional attribution heatmap overlay and patch highlight.

    Args:
        highlight: (x, y, width, height) rectangle to draw, or None.
    """
    plt.figure(figsize=(6, 6))
    ax = plt.gca()
    ax.imshow(img_array)
    if attribution is not None:
        ax.imshow(attribution, cmap='jet', alpha=0.3, interpolation='bilinear')
    if highlight is not None:
        hx, hy, hw, hh = highlight
        rect = mpatches.Rectangle(
            (hx, hy), hw, hh, linewidth=highlight_linewidth,
            edgecolor=highlight_color, facecolor='none'
        )
        ax.add_patch(rect)
    if title:
        ax.set_title(title, fontsize=14, fontweight='bold')
    ax.axis('off')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def _zscore(series: pd.Series) -> pd.Series:
    """Normalize series to zero mean, unit variance."""
    return (series - series.mean()) / (series.std() + 1e-9)


def load_and_preprocess_image(image_path: Path, dataset_config) -> np.ndarray:
    """
    Load and preprocess image using the EXACT same pipeline as the model.
    Uses dataset_config.get_transforms() - the single source of truth.

    Returns:
        Preprocessed image as numpy array [224, 224, 3] for visualization.
        Note: Will be normalized (not in [0,1] range), but suitable for visualization.
    """
    if not image_path.exists():
        return None

    img = Image.open(image_path).convert('RGB')

    # Use the EXACT same transforms as the model
    transform = dataset_config.get_transforms('test')
    img_tensor = transform(img)  # [3, 224, 224], normalized

    # Convert to numpy for visualization [224, 224, 3]
    img_array = img_tensor.permute(1, 2, 0).numpy()

    # Denormalize for visualization (reverse the normalization)
    # For CLIP: mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]
    # We'll handle this by clipping to reasonable range for display
    img_array = np.clip(img_array, -3, 3)  # Clip extreme values
    img_array = (img_array - img_array.min()) / (img_array.max() - img_array.min() + 1e-8)

    return img_array


def load_faithfulness_results(path: Path) -> pd.DataFrame:
    """Load per-image faithfulness results from experiment directory.

    Reads from the unified ``faithfulness_stats_*.json`` which contains
    per-image scores for all 3 metrics (SaCo, FaithfulnessCorrelation,
    PixelFlipping) and per-image classification metadata (``images`` array).
    """
    faithfulness_json = list(path.glob("faithfulness_stats_*.json"))
    if not faithfulness_json:
        raise FileNotFoundError(f"No faithfulness stats JSON found in {path}")

    with open(faithfulness_json[0], 'r') as f:
        stats = json.load(f)

    # Build DataFrame from per-image metadata
    images = stats.get('images', [])
    if images:
        df = pd.DataFrame(images)
    else:
        # Fallback: no per-image metadata, create minimal frame
        n_images = 0
        for m in stats.get('metrics', {}).values():
            if 'mean_scores' in m:
                n_images = len(m['mean_scores'])
                break
        df = pd.DataFrame({'image_idx': range(n_images)})

    # Add per-image metric scores
    for metric_name, metric_data in stats.get('metrics', {}).items():
        if 'mean_scores' in metric_data:
            scores = metric_data['mean_scores']
            if metric_name == 'SaCo':
                df['saco_score'] = scores
            else:
                df[metric_name] = scores

    # Extract image index from filename
    if 'filename' in df.columns and 'image_idx' not in df.columns:
        df['image_idx'] = df['filename'].str.extract(r'_(\d+)\.(?:jpeg|png)$')[0].astype(int)
    elif 'image_idx' not in df.columns:
        df['image_idx'] = range(len(df))

    return df


def load_debug_data(path: Path, layers: Optional[List[int]] = None) -> Dict[int, Dict[str, np.ndarray]]:
    """Load debug data containing sparse feature data and attribution deltas.

    Args:
        path: Path to experiment directory
        layers: Optional list of specific layers to load. If None, loads all layers.

    Expected NPZ keys (WP-18 format):
        patch_attribution_deltas, sparse_indices, sparse_activations, sparse_contributions
    """
    debug_dir = path / "debug"
    if not debug_dir.exists():
        raise FileNotFoundError(f"Debug data directory not found: {debug_dir}")

    debug_files = list(debug_dir.glob("layer_*_debug.npz"))
    if not debug_files:
        raise FileNotFoundError(f"No debug NPZ files found in {debug_dir}")

    debug_data = {}
    for debug_file in sorted(debug_files):
        layer_idx = int(debug_file.stem.split('_')[1])

        if layers is not None and layer_idx not in layers:
            continue

        data = np.load(debug_file, allow_pickle=True)

        debug_data[layer_idx] = {
            'sparse_indices': data['sparse_indices'],
            'sparse_activations': data['sparse_activations'],
            'sparse_contributions': data['sparse_contributions'],
            'patch_attribution_deltas': data['patch_attribution_deltas'],
        }

        print(f"  Layer {layer_idx}: {len(debug_data[layer_idx]['sparse_indices'])} images")

    return debug_data


def load_activation_data(path: Path, layers: Optional[List[int]] = None, max_images: Optional[int] = None) -> Dict[int, Dict[str, np.ndarray]]:
    """Load lightweight activation data (from extract_sae_activations.py).

    Args:
        path: Path to activations directory
        layers: Optional list of specific layers to load. If None, loads all layers.
        max_images: Optional limit on number of images to load (for memory savings).
    """
    debug_dir = path / "debug"
    if not debug_dir.exists():
        raise FileNotFoundError(f"Activation data directory not found: {debug_dir}")

    activation_files = list(debug_dir.glob("layer_*_activations.npz"))
    if not activation_files:
        raise FileNotFoundError(f"No activation NPZ files found in {debug_dir}")

    activation_data = {}
    for activation_file in sorted(activation_files):
        layer_idx = int(activation_file.stem.split('_')[1])

        # Skip if not in requested layers
        if layers is not None and layer_idx not in layers:
            continue

        data = np.load(activation_file, allow_pickle=True)

        sparse_indices = data['sparse_indices']
        sparse_activations = data['sparse_activations']

        # Limit to max_images if specified (saves RAM for large datasets)
        if max_images is not None and len(sparse_indices) > max_images:
            print(f"  Limiting to {max_images} images (out of {len(sparse_indices)}) to save RAM")
            sparse_indices = sparse_indices[:max_images]
            sparse_activations = sparse_activations[:max_images]

        activation_data[layer_idx] = {
            'sparse_indices': sparse_indices,
            'sparse_activations': sparse_activations,
        }

        print(f"  Layer {layer_idx}: {len(activation_data[layer_idx]['sparse_indices'])} images")

    return activation_data


def build_feature_activation_index(
    debug_data: Dict[int, Dict[str, np.ndarray]],
    layer_idx: int,
    source_name: str = "test",
    top_k_per_feature: int = 100,
    batch_size: int = 500
) -> Dict[int, List[Tuple[int, int, float]]]:
    """
    Build reverse index: feature_idx -> list of (debug_idx, patch_idx, activation).
    Memory-efficient version that processes in batches and only keeps top-K per feature.

    Note: debug_idx is the position in debug data arrays (0-N), not the actual image_idx.

    Args:
        debug_data: Debug data containing activations
        layer_idx: Layer to build index for
        source_name: Name of the source (e.g., "test", "validation") for logging
        top_k_per_feature: Only keep top K activations per feature (reduces memory)
        batch_size: Process images in batches to reduce memory usage
    """
    print(f"Building activation index for layer {layer_idx} from {source_name} set...")
    print(f"  Using batched processing (batch_size={batch_size}, keeping top-{top_k_per_feature} per feature)")

    feature_index = defaultdict(list)

    sparse_indices = debug_data[layer_idx]['sparse_indices']
    sparse_activations = debug_data[layer_idx]['sparse_activations']
    n_images = len(sparse_indices)

    # Process in batches (smaller batches = more frequent pruning = less peak memory)
    for batch_start in range(0, n_images, batch_size):
        batch_end = min(batch_start + batch_size, n_images)

        if batch_start % 5000 == 0:
            print(f"  Processing images {batch_start}-{batch_end}/{n_images}...")

        # Process this batch
        for debug_idx in range(batch_start, batch_end):
            for patch_idx in range(len(sparse_indices[debug_idx])):
                patch_features = sparse_indices[debug_idx][patch_idx]
                patch_activations = sparse_activations[debug_idx][patch_idx]

                for feat_idx, activation in zip(patch_features, patch_activations):
                    feature_index[int(feat_idx)].append((debug_idx, patch_idx, float(activation)))

        # After each batch, prune features that have grown too large
        # Keep 5x top_k during processing to preserve good candidates, final prune at end
        prune_threshold = top_k_per_feature * 5
        for feat_idx in list(feature_index.keys()):
            if len(feature_index[feat_idx]) > prune_threshold:
                feature_index[feat_idx] = sorted(feature_index[feat_idx], key=lambda x: x[2],
                                                 reverse=True)[:prune_threshold]

    # Final sort and prune to exact top-K
    print(f"  Finalizing index (keeping top-{top_k_per_feature} activations per feature)...")
    for feat_idx in feature_index:
        feature_index[feat_idx] = sorted(feature_index[feat_idx], key=lambda x: x[2], reverse=True)[:top_k_per_feature]

    print(f"  Indexed {len(feature_index)} unique features from {n_images} images")
    return dict(feature_index)


def compute_per_image_improvement(vanilla_df: pd.DataFrame, gated_df: pd.DataFrame) -> pd.DataFrame:
    """Compute composite improvement score from multiple metrics."""
    improvements = gated_df[['image_idx']].copy()

    # Calculate deltas
    improvements['delta_saco'] = gated_df['saco_score'] - vanilla_df['saco_score']
    improvements['delta_faith'] = gated_df['FaithfulnessCorrelation'] - vanilla_df['FaithfulnessCorrelation']
    improvements['delta_pixel'] = gated_df['PixelFlipping'] - vanilla_df['PixelFlipping']

    improvements['composite_improvement'] = (
        _zscore(improvements['delta_saco']) + _zscore(improvements['delta_faith']) +
        -_zscore(improvements['delta_pixel'])  # Lower is better for pixel flipping
    ) / 3

    return improvements


def find_dominant_features_in_image(
    debug_idx: int,
    image_idx: int,
    layer_idx: int,
    debug_data: Dict[int, Dict[str, np.ndarray]],
    vanilla_attr: np.ndarray,
    gated_attr: np.ndarray,
    n_top_patches: int = 5
) -> List[Dict[str, Any]]:
    """
    Find features that contributed most to attribution changes in an image.

    For each of the top-N patches (by |final_attribution_delta|), extract the
    feature with the strongest contribution in the matching direction.

    Args:
        debug_idx: Index in debug data arrays (not the actual image_idx)
        image_idx: Actual image index (for file lookup)
        vanilla_attr: Final vanilla attribution map [224, 224]
        gated_attr: Final gated attribution map [224, 224]
    """
    sparse_indices = debug_data[layer_idx]['sparse_indices'][debug_idx]
    sparse_contributions = debug_data[layer_idx]['sparse_contributions'][debug_idx]
    layer_patch_deltas = debug_data[layer_idx]['patch_attribution_deltas'][debug_idx]

    n_patches = len(layer_patch_deltas)
    patches_per_side, patch_size = _patch_grid(n_patches)

    final_patch_deltas = []
    for p_idx in range(n_patches):
        x, y = _patch_coords(p_idx, patches_per_side, patch_size)
        vanilla_patch_val = vanilla_attr[y:y + patch_size, x:x + patch_size].mean()
        gated_patch_val = gated_attr[y:y + patch_size, x:x + patch_size].mean()
        final_patch_deltas.append(gated_patch_val - vanilla_patch_val)

    final_patch_deltas = np.array(final_patch_deltas)

    # Find patches with largest absolute FINAL attribution changes
    top_patch_indices = np.argsort(np.abs(final_patch_deltas))[::-1][:n_top_patches]

    dominant_features = []
    skipped_reasons = {'negligible_delta': 0, 'no_features': 0, 'no_matching_direction': 0}

    for patch_idx in top_patch_indices:
        final_delta = final_patch_deltas[patch_idx]
        layer_delta = layer_patch_deltas[patch_idx]

        if abs(final_delta) < 1e-6:
            skipped_reasons['negligible_delta'] += 1
            continue

        patch_features = sparse_indices[patch_idx]
        patch_contributions = sparse_contributions[patch_idx]

        if len(patch_features) == 0:
            skipped_reasons['no_features'] += 1
            continue

        # Find feature with strongest contribution in matching direction
        sign = np.sign(final_delta)
        matching = [(i, c) for i, c in enumerate(patch_contributions) if np.sign(c) == sign]
        if not matching:
            skipped_reasons['no_matching_direction'] += 1
            continue
        best_idx, best_contrib = max(matching, key=lambda x: x[1] * sign)
        role = "BOOST" if sign > 0 else "SUPPRESS"

        feature_idx = int(patch_features[best_idx])

        dominant_features.append({
            'patch_idx': patch_idx,
            'final_patch_delta': float(final_delta),
            'layer_patch_delta': float(layer_delta),
            'feature_idx': feature_idx,
            'contribution': float(best_contrib),
            'role': role
        })

    # Debug: log if we filtered out all patches
    if sum(skipped_reasons.values()) > 0:
        print(f"    Warning: Image {image_idx} - all {n_top_patches} patches filtered. Reasons: {skipped_reasons}")

    return dominant_features


def extract_case_studies(
    vanilla_faithfulness: pd.DataFrame,
    gated_faithfulness: pd.DataFrame,
    debug_data: Dict[int, Dict[str, np.ndarray]],
    layer_idx: int,
    vanilla_attribution_dir: Path,
    gated_attribution_dir: Path,
    n_top_images: int = 100,
    n_patches_per_image: int = 5,
    mode: str = 'improved'
) -> pd.DataFrame:
    """
    Extract case studies from top improved or degraded images.

    Args:
        mode: Either 'improved' (default) or 'degraded'

    Returns a DataFrame with one row per (image, patch, feature) combination.
    """
    # Compute improvements
    improvements = compute_per_image_improvement(vanilla_faithfulness, gated_faithfulness)
    improvements = improvements.merge(
        gated_faithfulness[['image_idx', 'predicted_class', 'true_class', 'is_correct']], on='image_idx'
    )

    # IMPORTANT: Debug data is indexed by processing order (0-N), not by image_idx
    # We need to add a processing order index to match with debug data
    improvements['debug_idx'] = range(len(improvements))

    # Get top improved or degraded images based on mode
    if mode == 'degraded':
        top_images = improvements.nsmallest(n_top_images, 'composite_improvement')
        mode_label = 'degraded'
    else:
        top_images = improvements.nlargest(n_top_images, 'composite_improvement')
        mode_label = 'improved'

    print(f"\nAnalyzing top {n_top_images} {mode_label} images:")
    print(
        f"  Composite improvement range: [{top_images['composite_improvement'].min():.3f}, "
        f"{top_images['composite_improvement'].max():.3f}]"
    )

    # Extract dominant features from each image
    case_studies = []

    for _, img_row in top_images.iterrows():
        img_idx = img_row['image_idx']  # Actual image index (for file lookup)
        debug_idx = img_row['debug_idx']  # Index in debug data arrays

        # Load attribution maps
        vanilla_attr = load_attribution(img_idx, vanilla_attribution_dir)
        gated_attr = load_attribution(img_idx, gated_attribution_dir)

        if vanilla_attr is None or gated_attr is None:
            print(f"  Warning: Skipping image {img_idx} - attribution maps not found")
            continue

        dominant_features = find_dominant_features_in_image(
            debug_idx, img_idx, layer_idx, debug_data, vanilla_attr, gated_attr, n_top_patches=n_patches_per_image
        )

        for feat_info in dominant_features:
            case_studies.append({
                'image_idx': img_idx,
                'debug_idx': debug_idx,
                'composite_improvement': img_row['composite_improvement'],
                'delta_saco': img_row['delta_saco'],
                'delta_faith': img_row['delta_faith'],
                'delta_pixel': img_row['delta_pixel'],
                'predicted_class': img_row['predicted_class'],
                'true_class': img_row['true_class'],
                'is_correct': img_row['is_correct'],
                'layer_idx': layer_idx,
                **feat_info
            })

    case_studies_df = pd.DataFrame(case_studies)

    print(f"  Extracted {len(case_studies_df)} feature contributions")
    print(f"  Roles: {case_studies_df['role'].value_counts().to_dict()}")

    return case_studies_df


def _save_prototype_images(
    top_activations: List[Tuple[int, int, float]],
    debug_to_image_idx: Dict[int, int],
    prototype_path_mapping: Optional[Dict[int, Path]],
    prototype_image_dir: Path,
    dataset_config,
    patches_per_side: int,
    patch_size: int,
    case_dir: Path,
) -> List[Dict[str, Any]]:
    """Save prototype images with patch highlights. Returns prototype metadata list."""
    prototype_metadata = []
    for proto_idx, (proto_debug_idx, proto_patch_idx, proto_activation) in enumerate(top_activations):
        proto_img_idx = debug_to_image_idx.get(proto_debug_idx)
        if proto_img_idx is None:
            continue

        # Resolve prototype image path
        if prototype_path_mapping is not None:
            proto_img_path = prototype_path_mapping.get(proto_debug_idx)
            if proto_img_path is None:
                continue
        else:
            proto_img_path = get_image_path(proto_img_idx, prototype_image_dir)
            if proto_img_path is None:
                continue

        proto_img_array = load_and_preprocess_image(proto_img_path, dataset_config)
        if proto_img_array is None:
            continue

        px, py = _patch_coords(proto_patch_idx, patches_per_side, patch_size)
        _save_attribution_overlay(
            proto_img_array, case_dir / f"prototype_{proto_idx}.png",
            highlight=(px, py, patch_size, patch_size),
            highlight_color='yellow', highlight_linewidth=2,
            title=f"Activation={proto_activation:.2f}",
        )

        prototype_metadata.append({
            'prototype_idx': proto_idx,
            'image_idx': int(proto_img_idx),
            'debug_idx': int(proto_debug_idx),
            'patch_idx': int(proto_patch_idx),
            'activation': float(proto_activation),
            'image_path': str(proto_img_path),
        })
    return prototype_metadata


def _build_case_metadata(
    case: pd.Series,
    x: int, y: int,
    patches_per_side: int, patch_size: int,
    prototype_metadata: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """Build the metadata dict for a case study folder."""
    return {
        'image_idx': int(case['image_idx']),
        'layer_idx': int(case['layer_idx']),
        'feature_idx': int(case['feature_idx']),
        'patch_idx': int(case['patch_idx']),
        'role': case['role'],
        'patch_coordinates': {'x': x, 'y': y, 'width': patch_size, 'height': patch_size},
        'metrics': {
            'composite_improvement': float(case['composite_improvement']),
            'delta_saco': float(case['delta_saco']),
            'delta_faithfulness': float(case['delta_faith']),
            'delta_pixel_flipping': float(case['delta_pixel']),
            'final_patch_delta': float(case['final_patch_delta']),
            'layer_patch_delta': float(case['layer_patch_delta']),
            'feature_contribution': float(case['contribution']),
        },
        'classification': {
            'predicted_class': str(case['predicted_class']) if pd.notna(case['predicted_class']) else None,
            'true_class': str(case['true_class']) if pd.notna(case['true_class']) else None,
            'is_correct': bool(case['is_correct']),
        },
        'prototypes': prototype_metadata,
        'patch_config': {
            'patches_per_side': patches_per_side,
            'patch_size': patch_size,
            'total_patches': patches_per_side * patches_per_side,
        },
    }


def save_case_study_individual_images(
    case: pd.Series,
    feature_index: Dict[int, List[Tuple[int, int, float]]],
    debug_to_image_idx: Dict[int, int],
    dataset_config,
    case_study_image_dir: Path,
    prototype_image_dir: Path,
    vanilla_attribution_dir: Path,
    gated_attribution_dir: Path,
    output_dir: Path,
    patches_per_side: int,
    patch_size: int,
    n_prototypes: int = 10,
    prototype_path_mapping: Optional[Dict[int, Path]] = None,
):
    """Save case study as individual images in a folder with metadata JSON."""
    img_idx = case['image_idx']
    feature_idx = case['feature_idx']
    patch_idx = case['patch_idx']

    folder_name = (
        f"case_img{img_idx}_layer{case['layer_idx']}_feat{feature_idx}"
        f"_patch{patch_idx}_{case['role'].lower()}"
    )
    case_dir = output_dir / folder_name
    case_dir.mkdir(parents=True, exist_ok=True)

    # Load image, attributions
    image_path = get_image_path(img_idx, case_study_image_dir)
    if not image_path or not image_path.exists():
        print(f"Warning: Image {img_idx} not found")
        return

    vanilla_attr = load_attribution(img_idx, vanilla_attribution_dir)
    gated_attr = load_attribution(img_idx, gated_attribution_dir)
    if vanilla_attr is None or gated_attr is None:
        print(f"Warning: Attributions not found for image {img_idx}")
        return

    main_img = load_and_preprocess_image(image_path, dataset_config)
    if main_img is None:
        print(f"Warning: Could not load main image {image_path}")
        return

    x, y = _patch_coords(patch_idx, patches_per_side, patch_size)

    # Save attribution overlays
    vanilla_norm = (vanilla_attr - vanilla_attr.min()) / (vanilla_attr.max() - vanilla_attr.min() + 1e-8)
    gated_norm = (gated_attr - gated_attr.min()) / (gated_attr.max() - gated_attr.min() + 1e-8)

    _save_attribution_overlay(main_img, case_dir / "vanilla_attribution.png", attribution=vanilla_norm)
    _save_attribution_overlay(
        main_img, case_dir / "gated_attribution.png",
        attribution=gated_norm,
        highlight=(x, y, patch_size, patch_size),
        highlight_color='lime' if case['role'] == 'BOOST' else 'red',
        title=f"{case['role']} patch \u0394: {case['final_patch_delta']:.3f}",
    )

    # Save prototypes
    top_activations = feature_index.get(feature_idx, [])[:n_prototypes]
    if not top_activations and feature_idx in feature_index:
        print(f"Warning: Feature {feature_idx} not in index")

    prototype_metadata = _save_prototype_images(
        top_activations, debug_to_image_idx, prototype_path_mapping,
        prototype_image_dir, dataset_config, patches_per_side, patch_size, case_dir,
    )

    # Write metadata
    metadata = _build_case_metadata(case, x, y, patches_per_side, patch_size, prototype_metadata)
    with open(case_dir / "metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"  Saved: {folder_name}/")


def get_image_path(image_idx: int, image_dir: Path) -> Optional[Path]:
    """Get image path for ImageNet dataset."""
    image_path = image_dir / "class_-1" / f"img_-01_test_{image_idx:06d}.jpeg"
    return image_path if image_path.exists() else None


def load_attribution(image_idx: int, attribution_dir: Path) -> Optional[np.ndarray]:
    """Load attribution map for ImageNet dataset."""
    attr_path = attribution_dir / f"img_-01_test_{image_idx:06d}_attribution.npy"
    return np.load(attr_path) if attr_path.exists() else None


def _load_validation_prototype_mapping(activations_path: Path) -> Tuple[Dict[int, Path], Dict[int, int]]:
    """Load image path mapping from validation extraction metadata.

    Returns:
        (debug_to_path, debug_to_image_idx) mappings for the validation set.
    """
    metadata_file = activations_path / "extraction_metadata.json"
    if not metadata_file.exists():
        raise FileNotFoundError(
            f"Metadata file not found: {metadata_file}\n"
            f"Please re-run extract_sae_activations.py to generate the metadata with image path mapping."
        )

    with open(metadata_file, 'r') as f:
        metadata = json.load(f)

    if 'image_paths' not in metadata:
        raise ValueError(
            f"Metadata file is missing 'image_paths' field.\n"
            f"Please re-run extract_sae_activations.py to generate the correct metadata."
        )

    debug_to_path = {int(k): Path(v) for k, v in metadata['image_paths'].items()}
    debug_to_image_idx = {idx: idx for idx in debug_to_path.keys()}
    print(f"  Loaded mapping for {len(debug_to_path)} validation images from metadata")
    return debug_to_path, debug_to_image_idx


def _print_layer_summary(layer_idx: int, case_studies: pd.DataFrame):
    """Print summary statistics for a single layer's case studies."""
    print(f"\nLayer {layer_idx} Summary:")
    print(f"  Total case studies: {len(case_studies)}")
    print(f"  Unique features: {case_studies['feature_idx'].nunique()}")
    print(f"  Role distribution: {case_studies['role'].value_counts().to_dict()}")

    top_features = case_studies['feature_idx'].value_counts().head(10)
    print(f"\n  Most common features:")
    for feat_idx, count in top_features.items():
        avg_contrib = case_studies[case_studies['feature_idx'] == feat_idx]['contribution'].mean()
        print(f"    Feature {feat_idx}: appears {count}x, avg contribution {avg_contrib:.4f}")


def run_case_study_analysis(
    experiment_path: Path,
    experiment_config: str,
    dataset: str,
    layers: List[int],
    n_top_images: int = 20,
    n_patches_per_image: int = 5,
    n_case_visualizations: int = 10,
    n_prototypes: int = 10,
    validation_activations_path: Optional[Path] = None,
    mode: str = 'improved',
    max_prototype_images: Optional[int] = None
):
    """
    Main entry point for case study analysis.

    Args:
        experiment_path: Path to experiment directory
        experiment_config: Name of the experiment configuration
        dataset: Dataset name (e.g. 'imagenet', 'covidquex')
        layers: List of layer indices to analyze
        n_top_images: Number of top improved/degraded images to analyze
        n_patches_per_image: Number of patches per image to extract features from
        n_case_visualizations: Number of case studies to visualize
        n_prototypes: Number of prototype images to show per feature
        validation_activations_path: Optional path to validation set activations for prototypes.
                                     If provided, prototypes will be from validation set,
                                     otherwise they'll be from test set.
        max_prototype_images: Optional limit on validation images to load for prototypes (saves RAM).
        mode: Either 'improved' (default) or 'degraded' - selects top improved or degraded images
    """
    print("\n" + "=" * 80)
    print(f"CASE STUDY ANALYSIS: {dataset} / {experiment_config} ({mode})")
    print("=" * 80)

    # Set paths
    vanilla_path = experiment_path / dataset / "vanilla" / "test"
    gated_path = experiment_path / dataset / experiment_config / "test"
    output_subdir = f"case_studies_{mode}" if mode == 'degraded' else "case_studies"
    output_dir = experiment_path / output_subdir / experiment_config

    _validate_case_study_inputs(vanilla_path, gated_path, layers)

    print("Input paths:")
    print(f"  vanilla: {vanilla_path}")
    print(f"  gated:   {gated_path}")
    print("Output path:")
    print(f"  {output_dir}")
    print()

    # Load faithfulness results (lightweight - just CSV)
    print("Loading faithfulness results...")
    vanilla_faithfulness = load_faithfulness_results(vanilla_path)
    gated_faithfulness = load_faithfulness_results(gated_path)

    use_validation_prototypes = validation_activations_path is not None

    # Load dataset config for preprocessing
    from featuregating.datasets.dataset_config import get_dataset_config
    dataset_config = get_dataset_config(dataset)

    # Determine image directories
    test_image_dir = Path(f"./data/prepared/{dataset}/test")
    val_image_dir = Path(f"./data/prepared/{dataset}/val")

    # Create mapping from debug_idx to image_idx for test set
    # The debug data is in processing order, same as faithfulness CSV rows
    test_debug_to_image_idx = dict(enumerate(gated_faithfulness['image_idx'].tolist()))

    # Load validation prototype mapping if using validation prototypes
    if use_validation_prototypes:
        assert validation_activations_path is not None
        val_debug_to_path, val_debug_to_image_idx = _load_validation_prototype_mapping(validation_activations_path)
    else:
        val_debug_to_image_idx = None
        val_debug_to_path = None

    # Process each layer ONE AT A TIME to save RAM
    all_case_studies = []

    for layer_idx in layers:
        print(f"\n{'='*60}")
        print(f"LAYER {layer_idx}")
        print(f"{'='*60}")

        # Load data for THIS LAYER ONLY to save RAM
        print(f"Loading debug data for layer {layer_idx}...")
        debug_data = load_debug_data(gated_path, layers=[layer_idx])

        # Precompute patch grid from debug data
        sample_deltas = debug_data[layer_idx]['patch_attribution_deltas'][0]
        patches_per_side, patch_size = _patch_grid(len(sample_deltas))

        if use_validation_prototypes:
            print(f"Loading validation activations for layer {layer_idx}...")
            validation_data = load_activation_data(validation_activations_path, layers=[layer_idx], max_images=max_prototype_images)
        else:
            validation_data = None

        # Build feature activation index from validation or test data
        if use_validation_prototypes:
            assert validation_data is not None
            assert val_debug_to_image_idx is not None
            feature_index = build_feature_activation_index(validation_data, layer_idx, source_name="validation")
            prototype_debug_to_image_idx = val_debug_to_image_idx
            prototype_image_dir = val_image_dir
            prototype_paths = val_debug_to_path
        else:
            feature_index = build_feature_activation_index(debug_data, layer_idx, source_name="test")
            prototype_debug_to_image_idx = test_debug_to_image_idx
            prototype_image_dir = test_image_dir
            prototype_paths = None

        # Extract case studies (always from test set)
        case_studies = extract_case_studies(
            vanilla_faithfulness, gated_faithfulness, debug_data, layer_idx,
            vanilla_path / "attributions", gated_path / "attributions",
            n_top_images=n_top_images, n_patches_per_image=n_patches_per_image, mode=mode,
        )
        all_case_studies.append(case_studies)

        # Save case studies table
        layer_output_dir = output_dir / f"layer_{layer_idx}"
        layer_output_dir.mkdir(parents=True, exist_ok=True)
        case_studies.to_csv(layer_output_dir / "case_studies.csv", index=False)

        # Visualize top cases
        print(f"\nGenerating visualizations (saving individual images)...")
        for _, case in case_studies.head(n_case_visualizations).iterrows():
            save_case_study_individual_images(
                case, feature_index, prototype_debug_to_image_idx, dataset_config,
                test_image_dir, prototype_image_dir,
                vanilla_path / "attributions", gated_path / "attributions",
                layer_output_dir, patches_per_side, patch_size,
                n_prototypes=n_prototypes, prototype_path_mapping=prototype_paths,
            )

        _print_layer_summary(layer_idx, case_studies)

        # Free memory for this layer before processing next
        del debug_data, feature_index
        if validation_data is not None:
            del validation_data
        import gc
        gc.collect()

    # Save combined results
    combined_df = pd.concat(all_case_studies, ignore_index=True)
    combined_df.to_csv(output_dir / "all_case_studies.csv", index=False)

    print("\n" + "=" * 80)
    print("Case study analysis complete!")
    print(f"Results saved to: {output_dir}")
    print("=" * 80 + "\n")

    return combined_df


def main() -> None:
    """Config-first script entrypoint for case study analysis.

    Run:
        uv run python -m featuregating.experiments.case_studies
    """
    dataset = "imagenet"
    experiment_path, experiment_config = _discover_default_experiment(dataset)
    layers = _infer_layers_from_experiment_name(experiment_config)

    # Use validation set prototypes by default.
    validation_activations_path = _resolve_validation_activations_path(dataset, layers)

    print("Auto-selected experiment target:")
    print(f"  dataset:          {dataset}")
    print(f"  experiment_path:  {experiment_path}")
    print(f"  experiment_config:{experiment_config}")
    print(f"  layers:           {layers}")
    print(f"  prototype source: validation ({validation_activations_path})")
    print()

    run_case_study_analysis(
        experiment_path=experiment_path,
        experiment_config=experiment_config,
        dataset=dataset,
        layers=layers,
        n_top_images=50,
        n_patches_per_image=5,
        n_case_visualizations=30,
        n_prototypes=10,
        validation_activations_path=validation_activations_path,
        mode='improved',
        max_prototype_images=None,
    )

    run_case_study_analysis(
        experiment_path=experiment_path,
        experiment_config=experiment_config,
        dataset=dataset,
        layers=layers,
        n_top_images=50,
        n_patches_per_image=5,
        n_case_visualizations=30,
        n_prototypes=10,
        validation_activations_path=validation_activations_path,
        mode='degraded',
        max_prototype_images=None,
    )


if __name__ == "__main__":
    main()
